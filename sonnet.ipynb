{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7795209,"sourceType":"datasetVersion","datasetId":4563220},{"sourceId":11394,"sourceType":"modelInstanceVersion","modelInstanceId":8332}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":105.195288,"end_time":"2024-03-04T21:23:36.751958","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-04T21:21:51.55667","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"18992cc486ee4d22a3036d1a75250b3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d68a5b1d85241f0900f4d6df80889c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2fa3f36a3924a5b897362e98b645eca","placeholder":"​","style":"IPY_MODEL_18992cc486ee4d22a3036d1a75250b3f","value":" 0/4 [00:00&lt;?, ?it/s]"}},"2579fff4f50e460a87af3796b4bdfd45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd3baed25a2d492da4d67a30dd4b3248","placeholder":"​","style":"IPY_MODEL_87dae2f036fb4d7aa26ed80bac11b354","value":"Loading checkpoint shards:   0%"}},"26ecaa5e3b164e61a0eadea3d89e1da0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"715078b6c4234bdba68908e745d2f73e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2579fff4f50e460a87af3796b4bdfd45","IPY_MODEL_b825e9564e87472c931f0a443370385b","IPY_MODEL_1d68a5b1d85241f0900f4d6df80889c2"],"layout":"IPY_MODEL_b18d0417b2a444629ad5c4b439572196"}},"87dae2f036fb4d7aa26ed80bac11b354":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"933dd084f5324fd09955d6c7d062bef7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b18d0417b2a444629ad5c4b439572196":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b825e9564e87472c931f0a443370385b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_933dd084f5324fd09955d6c7d062bef7","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26ecaa5e3b164e61a0eadea3d89e1da0","value":0}},"bd3baed25a2d492da4d67a30dd4b3248":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2fa3f36a3924a5b897362e98b645eca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLMPR - Gemma prompt recovery\n\nAs a baseline let the LLM itself recover the prompt. \n\nI have seen such baseline already in other notebook titles, but I have not looked so far what prompt they use... so let's see. The model is wrapped in langchain for easier use.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.007108,"end_time":"2024-03-04T21:21:54.277664","exception":false,"start_time":"2024-03-04T21:21:54.270556","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nimport os\nimport pathlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","metadata":{"papermill":{"duration":0.81183,"end_time":"2024-03-04T21:21:55.096111","exception":false,"start_time":"2024-03-04T21:21:54.284281","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T01:44:18.996623Z","iopub.execute_input":"2024-03-15T01:44:18.997421Z","iopub.status.idle":"2024-03-15T01:44:19.415718Z","shell.execute_reply.started":"2024-03-15T01:44:18.997387Z","shell.execute_reply":"2024-03-15T01:44:19.414945Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Init Gemma","metadata":{"papermill":{"duration":0.006324,"end_time":"2024-03-04T21:21:55.109281","exception":false,"start_time":"2024-03-04T21:21:55.102957","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_path = \"/kaggle/input/gemma/transformers/7b-it/2\"\nprint(sorted(os.listdir(model_path)))\nprint(len(os.listdir(model_path)))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:45:02.294017Z","iopub.execute_input":"2024-03-15T01:45:02.294531Z","iopub.status.idle":"2024-03-15T01:45:02.308888Z","shell.execute_reply.started":"2024-03-15T01:45:02.294494Z","shell.execute_reply":"2024-03-15T01:45:02.308106Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['.gitattributes', 'config.json', 'generation_config.json', 'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors', 'model.safetensors.index.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config.json']\n10\n","output_type":"stream"}]},{"cell_type":"code","source":"assert len(os.listdir(model_path)) == 13, \"not all models files are present\"","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:44:22.624266Z","iopub.execute_input":"2024-03-15T01:44:22.624617Z","iopub.status.idle":"2024-03-15T01:44:22.681034Z","shell.execute_reply.started":"2024-03-15T01:44:22.624589Z","shell.execute_reply":"2024-03-15T01:44:22.679939Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(model_path)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m13\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot all models files are present\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mAssertionError\u001b[0m: not all models files are present"],"ename":"AssertionError","evalue":"not all models files are present","output_type":"error"}]},{"cell_type":"code","source":"!pip install --no-index --find-links /kaggle/input/llmpr-packages transformers==4.39.0.dev0 accelerate bitsandbytes langchain","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-15T01:30:00.686928Z","iopub.execute_input":"2024-03-15T01:30:00.687239Z","iopub.status.idle":"2024-03-15T01:30:13.650450Z","shell.execute_reply.started":"2024-03-15T01:30:00.687212Z","shell.execute_reply":"2024-03-15T01:30:13.649241Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Looking in links: /kaggle/input/llmpr-packages\nRequirement already satisfied: transformers==4.39.0.dev0 in /opt/conda/lib/python3.10/site-packages (4.39.0.dev0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.0)\nRequirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.1.11)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.25 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.27)\nRequirement already satisfied: langchain-core<0.2,>=0.1.29 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.30)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.1)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.23)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.2.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"papermill":{"duration":5.784496,"end_time":"2024-03-04T21:23:29.51234","exception":false,"start_time":"2024-03-04T21:23:23.727844","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T01:30:13.651919Z","iopub.execute_input":"2024-03-15T01:30:13.652244Z","iopub.status.idle":"2024-03-15T01:30:13.658771Z","shell.execute_reply.started":"2024-03-15T01:30:13.652214Z","shell.execute_reply":"2024-03-15T01:30:13.657008Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"4.39.0.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, \n    quantization_config=quantization_config\n)","metadata":{"papermill":{"duration":4.525249,"end_time":"2024-03-04T21:23:34.09309","exception":true,"start_time":"2024-03-04T21:23:29.567841","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T01:30:13.659858Z","iopub.execute_input":"2024-03-15T01:30:13.660120Z","iopub.status.idle":"2024-03-15T01:30:29.611117Z","shell.execute_reply.started":"2024-03-15T01:30:13.660098Z","shell.execute_reply":"2024-03-15T01:30:29.610351Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca63bbd6ed0a4198afe5dccbeb449d5b"}},"metadata":{}}]},{"cell_type":"code","source":"quantization_config","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:30:29.612244Z","iopub.execute_input":"2024-03-15T01:30:29.612555Z","iopub.status.idle":"2024-03-15T01:30:29.618803Z","shell.execute_reply.started":"2024-03-15T01:30:29.612530Z","shell.execute_reply":"2024-03-15T01:30:29.617827Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"BitsAndBytesConfig {\n  \"_load_in_4bit\": true,\n  \"_load_in_8bit\": false,\n  \"bnb_4bit_compute_dtype\": \"float32\",\n  \"bnb_4bit_quant_type\": \"fp4\",\n  \"bnb_4bit_use_double_quant\": false,\n  \"llm_int8_enable_fp32_cpu_offload\": false,\n  \"llm_int8_has_fp16_weight\": false,\n  \"llm_int8_skip_modules\": null,\n  \"llm_int8_threshold\": 6.0,\n  \"load_in_4bit\": true,\n  \"load_in_8bit\": false,\n  \"quant_method\": \"bitsandbytes\"\n}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T01:30:29.620046Z","iopub.execute_input":"2024-03-15T01:30:29.620347Z","iopub.status.idle":"2024-03-15T01:30:29.632749Z","shell.execute_reply.started":"2024-03-15T01:30:29.620294Z","shell.execute_reply":"2024-03-15T01:30:29.631958Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"GemmaTokenizerFast(name_or_path='/kaggle/input/gemma/transformers/7b-it/2', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"Write a haiku about lions.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=256)\nprint(tokenizer.decode(outputs[0]))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-03-15T01:30:29.633917Z","iopub.execute_input":"2024-03-15T01:30:29.634224Z","iopub.status.idle":"2024-03-15T01:30:31.422248Z","shell.execute_reply.started":"2024-03-15T01:30:29.634200Z","shell.execute_reply":"2024-03-15T01:30:31.421329Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<bos>Write a haiku about lions.\n\nGolden mane ablaze,\nRoaming through the savanna,\nKing's grace in stride.<eos>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# langchain\n- See https://python.langchain.com/docs/modules/model_io/llms/custom_llm\n- Effectively only ``_call`` methods is required.","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"import langchain\nprint(langchain.__version__)\nTOKENIZERS_PARALLELISM= False","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:02.766749Z","iopub.execute_input":"2024-03-15T01:31:02.767099Z","iopub.status.idle":"2024-03-15T01:31:02.772216Z","shell.execute_reply.started":"2024-03-15T01:31:02.767074Z","shell.execute_reply":"2024-03-15T01:31:02.771273Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"0.1.11\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Any, List, Mapping, Optional\nfrom langchain_core.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:05.813841Z","iopub.execute_input":"2024-03-15T01:31:05.814753Z","iopub.status.idle":"2024-03-15T01:31:05.819610Z","shell.execute_reply.started":"2024-03-15T01:31:05.814719Z","shell.execute_reply":"2024-03-15T01:31:05.818577Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class GemmaLLM(LLM):\n    cfg: Any\n    \n    \n    def __init__(self, max_new_tokens=256, **kwargs):\n        super().__init__(**kwargs)\n        # resets generation config\n        cfg = model.generation_config.from_pretrained(model_path)\n        cfg.max_new_tokens = max_new_tokens\n        for k, v in kwargs.items():\n            if hasattr(cfg, k):\n                setattr(cfg, k, v)\n        self.cfg = cfg\n        \n\n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        outputs = model.generate(**input_ids, generation_config=self.cfg)\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        remove_keys = [\"pad_token_id\", \"bos_token_id\", \"eos_token_id\", '_from_model_config', \"transformers_version\"]\n        return {k: v for k, v in self.cfg.to_diff_dict().items() if k not in remove_keys}\n    \n    def __repr__(self):\n        params = \", \".join(f\"{k}={v}\" for k, v in self._identifying_params.items())\n        return f\"Gemma({params})\"","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:10.094360Z","iopub.execute_input":"2024-03-15T01:31:10.094734Z","iopub.status.idle":"2024-03-15T01:31:10.108042Z","shell.execute_reply.started":"2024-03-15T01:31:10.094705Z","shell.execute_reply":"2024-03-15T01:31:10.106998Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"llm = GemmaLLM(temperature=1.0, do_sample=True)\nllm","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:11.733225Z","iopub.execute_input":"2024-03-15T01:31:11.733603Z","iopub.status.idle":"2024-03-15T01:31:11.744831Z","shell.execute_reply.started":"2024-03-15T01:31:11.733576Z","shell.execute_reply":"2024-03-15T01:31:11.743814Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"Gemma(max_new_tokens=256, do_sample=True)"},"metadata":{}}]},{"cell_type":"code","source":"prompt = \"Write a haiku about the Alps.\"\nout = llm.invoke(prompt)\nprint(out)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:13.138581Z","iopub.execute_input":"2024-03-15T01:31:13.138947Z","iopub.status.idle":"2024-03-15T01:31:14.998540Z","shell.execute_reply.started":"2024-03-15T01:31:13.138920Z","shell.execute_reply":"2024-03-15T01:31:14.997625Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Write a haiku about the Alps.\n\nSnowcapped peaks reach high,\nGranite walls soar to the sky,\nWinter's icy grip.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Ask for the rewrite prompt","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\ndata","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:16.245907Z","iopub.execute_input":"2024-03-15T01:31:16.246283Z","iopub.status.idle":"2024-03-15T01:31:16.260991Z","shell.execute_reply.started":"2024-03-15T01:31:16.246253Z","shell.execute_reply":"2024-03-15T01:31:16.259840Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"   id                                      original_text  \\\n0  -1  The competition dataset comprises text passage...   \n\n                                      rewritten_text  \n0  Here is your shanty: (Verse 1) The text is rew...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>original_text</th>\n      <th>rewritten_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>The competition dataset comprises text passage...</td>\n      <td>Here is your shanty: (Verse 1) The text is rew...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"rec = data.to_dict(orient=\"records\")[0]\nrec","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:24.035266Z","iopub.execute_input":"2024-03-15T01:31:24.035666Z","iopub.status.idle":"2024-03-15T01:31:24.042823Z","shell.execute_reply.started":"2024-03-15T01:31:24.035637Z","shell.execute_reply":"2024-03-15T01:31:24.041818Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'id': -1,\n 'original_text': 'The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.',\n 'rewritten_text': \"Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\"}"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:25.242809Z","iopub.execute_input":"2024-03-15T01:31:25.243154Z","iopub.status.idle":"2024-03-15T01:31:25.247368Z","shell.execute_reply.started":"2024-03-15T01:31:25.243126Z","shell.execute_reply":"2024-03-15T01:31:25.246464Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"temp = \"\"\"\nYou are an assistant to generate prompt templates for LLM.\nHere is an original text and a rewritten text.\nThink about a short and concise rewrite prompt that would make a LLM together with the original text provide the rewritten text.\nThis prompt is at most 3 sentences long. \nOnly return this rewrite prompt.\n\nOriginal text:::\n{original_text}\n\nRewritten text:::\n{rewritten_text}\n\nRewrite prompt:::\n\"\"\"\ntemp = PromptTemplate.from_template(temp)\ntemp","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:26.902293Z","iopub.execute_input":"2024-03-15T01:31:26.902702Z","iopub.status.idle":"2024-03-15T01:31:26.909722Z","shell.execute_reply.started":"2024-03-15T01:31:26.902675Z","shell.execute_reply":"2024-03-15T01:31:26.908681Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"PromptTemplate(input_variables=['original_text', 'rewritten_text'], template='\\nYou are an assistant to generate prompt templates for LLM.\\nHere is an original text and a rewritten text.\\nThink about a short and concise rewrite prompt that would make a LLM together with the original text provide the rewritten text.\\nThis prompt is at most 3 sentences long. \\nOnly return this rewrite prompt.\\n\\nOriginal text:::\\n{original_text}\\n\\nRewritten text:::\\n{rewritten_text}\\n\\nRewrite prompt:::\\n')"},"metadata":{}}]},{"cell_type":"code","source":"print(temp.format(**rec))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:27.954326Z","iopub.execute_input":"2024-03-15T01:31:27.954718Z","iopub.status.idle":"2024-03-15T01:31:27.960057Z","shell.execute_reply.started":"2024-03-15T01:31:27.954690Z","shell.execute_reply":"2024-03-15T01:31:27.959089Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"\nYou are an assistant to generate prompt templates for LLM.\nHere is an original text and a rewritten text.\nThink about a short and concise rewrite prompt that would make a LLM together with the original text provide the rewritten text.\nThis prompt is at most 3 sentences long. \nOnly return this rewrite prompt.\n\nOriginal text:::\nThe competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n\nRewritten text:::\nHere is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n\nRewrite prompt:::\n\n","output_type":"stream"}]},{"cell_type":"code","source":"chain = temp | llm","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:29.277995Z","iopub.execute_input":"2024-03-15T01:31:29.278387Z","iopub.status.idle":"2024-03-15T01:31:29.282744Z","shell.execute_reply.started":"2024-03-15T01:31:29.278359Z","shell.execute_reply":"2024-03-15T01:31:29.281817Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"out = chain.invoke(rec)\nprint(out)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:30.211863Z","iopub.execute_input":"2024-03-15T01:31:30.212526Z","iopub.status.idle":"2024-03-15T01:31:38.890053Z","shell.execute_reply.started":"2024-03-15T01:31:30.212491Z","shell.execute_reply":"2024-03-15T01:31:38.889111Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"\nYou are an assistant to generate prompt templates for LLM.\nHere is an original text and a rewritten text.\nThink about a short and concise rewrite prompt that would make a LLM together with the original text provide the rewritten text.\nThis prompt is at most 3 sentences long. \nOnly return this rewrite prompt.\n\nOriginal text:::\nThe competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n\nRewritten text:::\nHere is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n\nRewrite prompt:::\nSure, here is the prompt:\n\n**Rewrite the original text provided below into a creative and engaging story.**\n\nPlease provide the original text below:\n\n(Original text)\n\nThe competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.\n\n\n**Note:** This is a code competition, so please expect the test data to be replaced with the full test set once your submission is scored.\n","output_type":"stream"}]},{"cell_type":"code","source":"out.split(\"Rewrite prompt:::\")[-1].strip()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:38.891838Z","iopub.execute_input":"2024-03-15T01:31:38.892290Z","iopub.status.idle":"2024-03-15T01:31:38.898448Z","shell.execute_reply.started":"2024-03-15T01:31:38.892255Z","shell.execute_reply":"2024-03-15T01:31:38.897611Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"'Sure, here is the prompt:\\n\\n**Rewrite the original text provided below into a creative and engaging story.**\\n\\nPlease provide the original text below:\\n\\n(Original text)\\n\\nThe competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.\\n\\n\\n**Note:** This is a code competition, so please expect the test data to be replaced with the full test set once your submission is scored.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Invoke","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/sample_submission.csv\")\nsub","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:41.486660Z","iopub.execute_input":"2024-03-15T01:31:41.487417Z","iopub.status.idle":"2024-03-15T01:31:41.504029Z","shell.execute_reply.started":"2024-03-15T01:31:41.487382Z","shell.execute_reply":"2024-03-15T01:31:41.503159Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"        id      rewrite_prompt\n0  9559194  Improve that text.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9559194</td>\n      <td>Improve that text.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"default_rewrite = \"Improve that text.\"","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:31:43.100745Z","iopub.execute_input":"2024-03-15T01:31:43.101583Z","iopub.status.idle":"2024-03-15T01:31:43.105271Z","shell.execute_reply.started":"2024-03-15T01:31:43.101551Z","shell.execute_reply":"2024-03-15T01:31:43.104425Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"results = []\nfor rec in data.to_dict(orient=\"records\"):\n    out = chain.invoke(rec)\n    res = out.split(\"Rewrite prompt:::\")\n    if len(res) != 2:\n        res = default_rewrite\n    else:\n        res = res[-1].strip()\n    results.append({\n        \"id\": rec[\"id\"],\n        \"rewrite_prompt\": res\n    })","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:33:17.353374Z","iopub.execute_input":"2024-03-15T01:33:17.353761Z","iopub.status.idle":"2024-03-15T01:33:21.766946Z","shell.execute_reply.started":"2024-03-15T01:33:17.353732Z","shell.execute_reply":"2024-03-15T01:33:21.766165Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(results)\nsub","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:33:23.559742Z","iopub.execute_input":"2024-03-15T01:33:23.560136Z","iopub.status.idle":"2024-03-15T01:33:23.569414Z","shell.execute_reply.started":"2024-03-15T01:33:23.560107Z","shell.execute_reply":"2024-03-15T01:33:23.568425Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"   id                                     rewrite_prompt\n0  -1  Sure, here is the prompt:\\n\\n**Rewrite the abo...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>Sure, here is the prompt:\\n\\n**Rewrite the abo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T01:33:24.927789Z","iopub.execute_input":"2024-03-15T01:33:24.928491Z","iopub.status.idle":"2024-03-15T01:33:24.933681Z","shell.execute_reply.started":"2024-03-15T01:33:24.928461Z","shell.execute_reply":"2024-03-15T01:33:24.932687Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}